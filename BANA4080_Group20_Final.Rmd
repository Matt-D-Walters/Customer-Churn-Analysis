---
title: Group 20 Final Project
author: "Matthew Walters, Duc Anh Pham, Anthony Blue"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

<div style="text-align: center;">
  <img src="regorK.png" alt="regorK" style="width:200px;height:auto;"/>
  <h2>**Forecasting Customer Churn in Regork's Telecom Expansion**</h2>
   <img src="ML.png" alt="regorK" style="width:400px;height:auto;"/>
</div>
##  {.tabset .tabset-fade .tabset-pills}


### Introduction

<center><h2> **Introduction** </h2> </center>
-  At Regork, we are trying to be differentiators and test out our waters in the telecommunications sector. This means we need to understand how our customers are using and leaving our services. This is important for our company because it can help us understand the most influential reasons why people are no longer using our services. This analysis will allow you to help understand our current customers as well as know what resonates well with our loyal customers so we can try and replicate that moving forward when marketing to a new customer base.
<center><h2> **Methodology** </h2> </center>

- For our methodology, we will be using machine learning to help identify what the most important features are that might cause someone to leave. The machine learning models that we will be evaluating are logistic regression, multivariate adaptive regression splines (MARS), and random forest. We will assess these models by looking at the area under curve (AUC) and evaluating the confusion matrix for each model. 


<center><h2> **Proposed Solution** </h2></center>

### Packages Required

We show the packages used below:

- **tidyverse:** Used to tidy up our data
- **tidymodels:** Allows us to do our ML workflow
- **dplyr:** Applied for data manipulation and transformation
- **ggplot2:** Vital with creating visually appealing graphs
- **DT:** Create visually appealing tabular data
- **gridExtra:** Allows us to plot in a grid format
- **pdp:** Allows us to plot partial dependencies 
- **earth:** Allows us to do multivariate adaptive regression splines
- **vip** Used to plot feature importance

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(DT)
library(gridExtra)
library(pdp)
library(earth)
library(vip)

```

### Data Preparation {.tabset .tabset-fade .tabset-pills}
#### Overview of  Our Data Set
<h2> **Showcase a Sample of the Retention Data** </h2>
```{r, message=FALSE, warning=FALSE}
retention <- read.csv("data/customer_retention.csv")

# Pull in our retention dataset
datatable(head(retention, 15, width = auto))
```

          
#### Transformation / Meta Data
Below is an overview of the metadata and transformations used in this dataset:

- **Transformations**: 
    - Changed the Status variable into a factor
    - Dropped any null values from our dataset


<h3> Retention </h3>
```{r, message=FALSE, warning=FALSE}

retention <- retention %>% 
  dplyr::mutate(Status = as.factor(Status))

retention <- drop_na(retention)

glimpse(retention) 

print(colSums(is.na(retention))) 




```




### Exploratory Data Analysis

<h1> What Contract Types have the Highest Attrition Rates?</h1>
```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
Regork_colors <- c("#0071ce", "#4b7f42") 

# Retention Rate By Contract Type
ggplot(retention, aes(x = Contract, fill = Status)) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = Regork_colors) +
  theme_minimal() +
  labs(y = "Retention Rate (%)", 
       title = "Retention Rate by Contract Type") +
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 10),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(size = 18, face = "bold", hjust = 0.5))

```  

Looking at the different types of contracts that we offer (**month-to-month**, *one-year*, and *two-year*), it looks like people who are on a **month-to-month** contract are more likely to leave as opposed to those who are on a *one-year* or *two-year* contract.


<h1> How Does Retention Rate Vary Among Customers Based on TV and Internet Service? </h1>
```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
retention %>%
  group_by(StreamingTV, Status) %>%
  ggplot(aes(x = StreamingTV, fill = Status)) + 
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = Regork_colors) +  
  labs(y = "Retention Rate (%)", 
       title = "Retention Rate Based on TV Streaming Package")  +
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 10),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(size = 18, face = "bold", hjust = 0.5))
```
This graph shows the **retention rate** of those with the *streaming TV service*, without the *TV service*, and those with the *TV service* but not the *internet service*. Those three categories are then broken into two proportions, *current customers* and those that have *left*.

One insight that can be pulled from the graph is that the proportion of **current customers** to **leaving customers** is nearly identical for those with internet and TV streaming compared to those with internet but no TV Streaming.

<h1> How is customer status distributed based on whether they're senior citizens or not?</h1>

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}

plot2data <- retention %>% 
  mutate( 
    Gender = as.factor(Gender), 
    SeniorCitizen = factor(SeniorCitizen, levels = c(0, 1), labels = c("No", "Yes")), 
    Partner = as.factor(Partner), 
    Dependents =	 as.factor(Dependents), 
    PhoneService = as.factor(PhoneService), 
    MultipleLines = as.factor(MultipleLines), 
    Status = as.factor(Status) 
  ) 

ggplot(plot2data, aes(x = SeniorCitizen, fill = Status)) + 
  geom_bar(position = "dodge") + 
  labs(x = "Senior Citizen?", y = "Count", fill = "Status") + 
  scale_fill_manual(values = Regork_colors) + 
  ggtitle("Distribution of customer status\nbased on whether they were a senior citizen") + 
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 10),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(size = 18, face = "bold", hjust = 0.5))
```

From the graph, we can see that there is a larger number of **current customers** who are *not senior citizens* compared to those who are. The number of **customers who have churned** is lower than the number of **current customers** for all categories. The difference between the number of **current** and **churned customers** is larger among *non-senior citizens* than among *senior citizens*, suggesting that the churn proportion is much larger for the number of *senior citizens*. However, the *customer segment* of *non-senior citizens* certainly outweighs the *senior one*.




### Machine Learning {.tabset .tabset-fade .tabset-pills}

#### Machine Learning Data Preprocessing
<h1> Preprocessing Steps </h1>
- **Data Spliting:** First, we are going to split our dataset with a 70/30 split. 70% is going to be allocated to our training data set, while 30% will be used for our testing data set, to assess how well our data responds to unseen data.
- **K-Fold Cross-Validation:** Next, we set up our k-folds validation, which allows us to assess the model's generalization ability and helps mitigate the risks of overfitting or underfitting to a particular train-test split.
- **Data Preparation with Recipe:** Last, we created a recipe to normalize all of our numeric predictors. Then, we dummy encoded all of our categorical predictors to allow our machine learning models to incorporate them.

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
set.seed(123)
split <- initial_split(retention, prop = 0.7, strata = "Status")
retention_train <- training(split)
retention_test <- testing(split)


set.seed(123)
kfolds <- vfold_cv(retention_train, v = 5, strata = Status)


retention_recipe <- recipe(Status ~ ., data = retention_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

```




#### Logistic Regression Model
<h1> Setting up our Logstic Regression model </h1>
```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}

logistic_results <- logistic_reg() %>%
  fit_resamples(Status ~ ., kfolds)
collect_metrics(logistic_results)
```
Let's define our **Logistic regression model** . Then, we will asses the model based on the  **ROC** from our model, which is **0.845**!

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
logistic_final_fit <- logistic_reg() %>%
  fit(Status ~ ., data = retention_train) 

```

***This code will tell us which factors had the greatest influence on the customer status.***


```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
logistic_final_fit %>%
  predict(retention_test) %>%
  bind_cols(retention_test %>% select(Status)) %>%
  conf_mat(truth = Status, estimate = .pred_class)

vip(logistic_final_fit$fit, num_features = 20)
```
**_What are the most important features for this logistic regression model?_**


Also, this code creates a **confusion matrix**, which allows us to see which predictions the model got **correct** and which predictions it got **wrong** on our testing set.



#### Multivariate Adaptive Regression Splines (MARS) Model

<h1> Setting up our MARS model </h1>
```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}

mars_mod <- mars(num_terms = tune(), prod_degree = tune()) %>%
  set_mode("classification")

mars_grid <- grid_regular(num_terms(range = c(1,100)), prod_degree(), levels = 25)

retention_wf <- workflow() %>%
  add_recipe(retention_recipe) %>%
  add_model(mars_mod)

tuning_results <- retention_wf %>%
  tune_grid(resamples = kfolds, grid = mars_grid)

tuning_results %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  arrange(desc(mean))

```

Let's define our **MARS model** for classification and use a grid to tune our hyperparameters. From there, we will collect the best **ROC** from our model, which is **0.849**!


```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
best_hyperparameters <- select_best(tuning_results, metric = "roc_auc")

mars_final_wf <- workflow() %>%
  add_recipe(retention_recipe) %>%
  add_model(mars_mod) %>%
  finalize_workflow(best_hyperparameters)


mars_final_fit <- mars_final_wf %>%
  fit(data = retention_train)

# Predicting on our test set
mars_final_fit %>%
  predict(retention_test) %>%
  bind_cols(retention_test %>% select(Status)) %>%
  conf_mat(truth = Status, estimate = .pred_class)

mars_final_fit %>%
  predict(retention_test, type = "prob") %>%
  mutate(truth = retention_test$Status) %>%
  roc_auc(truth, .pred_Current)


```

**Proceeding** with fitting the final workflow to the training data, we then *predict* on the test set, *evaluate* model performance using a confusion matrix, and *calculate* the AUC score. 

The results of the model prediction on the test set show that out of the instances labeled as "Current" the model correctly predicted **1383** people that are current and incorrectly classified **242** people as "Left". Similarly, for instances labeled as "Left" the model correctly predicted **315** people that have left and misclassified **157** people as "Current".

Additionally, the AUC score is calculated as **0.847** which is similar to our training AUC.


```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
mars_final_fit %>%
  extract_fit_parsnip() %>%
  vip()
```

We can see here that our most important features on why someone is likely to leave are their **total charges**, the *tenure* of their account, and the **monthly charges** they acquire.




#### Random Forest Model

<h1> Setting up our Random Forest model </h1>
First, we trained our model with default hyperparameter values and used the same 5-fold cross validation object and achieved the mean 5-fold cross-validated AUC of **0.836**. After that, we arrived at the optimal random forest model with the highest AUC score of **0.846** after tuning the following values: 

- **trees**: tune of values ranging from 15 to 300 (From the rule of thumb of starting with 10 times the number of features) 
- **mtry**: tune for values ranging from 2 to 12 (Starting with five evenly spaced values of mtry across the range 2-p centered at the recommended default  √p) 
- **min_n**: tune for values ranging from 1 to 20 

Assess a total of **5 values** from each parameter (*levels = 5*).

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}

rf_mod <- rand_forest( 
  mode = "classification", 
  trees = tune(), 
  mtry = tune(), 
  min_n = tune() 
  ) %>% 
  set_engine("ranger", importance = "impurity") 
# create the hyperparameter grid 

rf_hyper_grid <- grid_regular( 
  trees(range = c(15, 300)), 
  mtry(range = c(2, 12)), 
  min_n(range = c(1, 20)), 
  levels = 5 
  ) 

# train our model across the hyper parameter grid 

set.seed(123) 
rf_results <- tune_grid(rf_mod, retention_recipe, resamples = kfolds, grid = rf_hyper_grid) 
# model results 

show_best(rf_results, metric = "roc_auc") 

```


```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
rf_best_hyperparameters <- select_best(rf_results, metric = "roc_auc")

rf_final_wf <- workflow() %>%
  add_recipe(retention_recipe) %>%
  add_model(rf_mod) %>%
  finalize_workflow(rf_best_hyperparameters)


rf_final_fit <- rf_final_wf %>%
  fit(data = retention_train)

# Predicting on our test set
rf_final_fit %>%
  predict(retention_test) %>%
  bind_cols(retention_test %>% select(Status)) %>%
  conf_mat(truth = Status, estimate = .pred_class)

rf_final_fit %>%
  predict(retention_test, type = "prob") %>%
  mutate(truth = retention_test$Status) %>%
  roc_auc(truth, .pred_Current)
```
 
 This random forest model demonstrates a **stronger ability** to identify customers who are still with the service rather than those who have left. The higher number of *false positives*, where the model predicts customers as "Current" when they have actually "Left," indicates a tendency of the model to be overly optimistic about customer retention. Conversely, the lower number of *false negatives* suggests that the model is more cautious about predicting that a customer has churned. This pattern reveals a bias in the model towards predicting customer continuation, which may lead to an underestimation of the churn rate.

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
rf_final_fit %>%  
   predict(retention_train, type = "prob") %>% 
   mutate(truth = retention_train$Status) %>% 
   roc_curve(truth, .pred_Current) %>% 
   autoplot() 
```
 
### Summary {.tabset}

In terms of **relative importance**, how would you rate the predictors in your model? As a business manager, which factors would you focus on (for example, you could invest in offering some incentives or promotions) to *decrease* the chances of customers leaving?

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}
mars_final_fit %>%
  extract_fit_parsnip() %>%
  vip()
```

In our optimal model, the most influential variables are ***'Total Charges'***, ***'Tenure'***, and ***'MonthlyCharges'***. Following those variables are ***'PaymentMethod_Electronic.check'*** and ***'OnlineSecurity_Yes'***.


As a business manager, we would focus on understanding why these predictors are influential. Firstly, if **high total charges** and **monthly** charges are a primary reason for customers leaving, we'd look into restructuring pricing or providing incentives/programs that could potentially provide additional value to our customers. If customers with **longer tenure** are more loyal, we’d explore rewards for customer loyalty. Additionally, improving service quality in the area of **tech support** and **online security** could address specific concerns that lead to customer churn. Offering incentives or promotions tied to **electronic payment methods**, **long-term contracts**, and **bundled services** with phone and online backup could also be effective strategies to decrease churn.

Customers that would leave:
```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}

customers_left <- mars_final_fit %>%
  predict(retention_test) %>%
  bind_cols(retention_test) %>% 
  filter(.pred_class == "Left")

customers_left
```

The predicted revenue loss if not action is taken:
```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=10}

customers_left %>%
  summarize(lost_revenue = sum(MonthlyCharges))
```